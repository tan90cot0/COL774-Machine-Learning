{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb4995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7aab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*sigmoid(1-x)\n",
    "\n",
    "def relu_func(x):\n",
    "    return x*(x>0)\n",
    "\n",
    "#check this\n",
    "def relu_der(x):\n",
    "    return 1*(x>0)\n",
    "\n",
    "def cost_function(a,y):\n",
    "    m = y.shape[1]\n",
    "    matrix = (y-a)*(y-a)\n",
    "    return (1/(2*m))*(np.sum(matrix))\n",
    "\n",
    "def get_data(path):\n",
    "    df = pd.read_csv (path, header=None)\n",
    "    pixel_values = df.iloc[0:df.shape[0],0:df.shape[1]-1]\n",
    "    pixel_values = np.array(pixel_values)\n",
    "\n",
    "    #Data normalization\n",
    "    max_pixel_value = np.max(pixel_values)\n",
    "    pixel_values = pixel_values/max_pixel_value\n",
    "\n",
    "    temp = df.iloc[:,df.shape[1]-1]\n",
    "    temp = np.array(temp)\n",
    "\n",
    "    output = np.zeros((df.shape[0],10))\n",
    "    for i in range(10):\n",
    "        output[np.where(temp==i),i]=1\n",
    "    return pixel_values.T, output.T\n",
    "\n",
    "def init_params(layer):\n",
    "    W = []\n",
    "    b = []\n",
    "    for i in range(len(layer)-1):\n",
    "        np.random.seed(i)\n",
    "        W.append(np.random.randn(layer[i+1],layer[i])*0.01)\n",
    "        b.append(np.zeros((layer[i+1],1)))\n",
    "    return W,b\n",
    "\n",
    "def minibatch_create(X, y, r):\n",
    "    X = X.T\n",
    "    y = y.T\n",
    "    minibatches = []\n",
    "    joint = np.hstack((X, y))\n",
    "    #to randomise the combined dataset\n",
    "    np.random.shuffle(joint)\n",
    "    for i in range(joint.shape[0]//r):\n",
    "        minibatch = joint[i * r:(i + 1)*r, :]\n",
    "        new_X = minibatch[:, :-10].T\n",
    "        new_y = minibatch[:, -10:].reshape((-1, 10)).T\n",
    "        minibatches.append((new_X, new_y))\n",
    "    return minibatches\n",
    "\n",
    "def forward_prop(params,X, relu):\n",
    "    W,b = params\n",
    "    a = [X]\n",
    "    z = []\n",
    "    for i in range(len(b)):\n",
    "        z.append(W[i]@a[i] + b[i])\n",
    "        if relu==True and i<len(b)-1:\n",
    "            a.append(relu_func(z[i]))\n",
    "        else:\n",
    "            a.append(sigmoid(z[i]))\n",
    "    return z,a\n",
    "\n",
    "def derivative(z, relu, last_layer):\n",
    "    if relu==True and last_layer == False:\n",
    "        return relu_der(z)\n",
    "    else:\n",
    "        return sigmoid_der(z)\n",
    "    \n",
    "def outer_loss(mse, y, al):\n",
    "    if mse:\n",
    "        return al-y\n",
    "    else:\n",
    "        d_al = -(np.divide(y, al) - np.divide(1-y, 1-al))\n",
    "        return d_al\n",
    "        \n",
    "\n",
    "def back_prop(params, z, a, y, relu, mse):\n",
    "    W,b = params\n",
    "    delta = []\n",
    "    for i in range(len(b)):\n",
    "        delta.append(None)\n",
    "    grads = {'W':[], 'b':[]}\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    for i in range(len(b)-1,-1,-1): \n",
    "        dA = outer_loss(mse, y, a[i+1]) if i==len(b)-1 else np.dot(W[i+1].T, delta[i+1])\n",
    "        delta[i] = dA * derivative(z[i], relu, i==len(b)-1) \n",
    "        grads['W'].append(np.dot(delta[i], a[i].T)/m)\n",
    "        grads['b'].append(np.sum(delta[i], axis=1, keepdims=True)/m)\n",
    "    grads['W'].reverse()\n",
    "    grads['b'].reverse()\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update(params,grads,alpha):\n",
    "    W,b = params\n",
    "    for i in range(len(W)):\n",
    "        W[i] = W[i] - alpha*grads['W'][i]\n",
    "        b[i] = b[i] - alpha*grads['b'][i]\n",
    "    return W,b\n",
    "\n",
    "def accuracy(y,y_hat):\n",
    "    matrix = y - y_hat\n",
    "    positive = 0\n",
    "    for i in range(y.shape[1]):\n",
    "        if(np.max(matrix[:,i])==0):\n",
    "            positive = positive + 1\n",
    "    return (positive/y.shape[1])*100\n",
    "\n",
    "def model(max_epochs, minibatches, params, alpha, adaptive, conv, relu, mse):\n",
    "    cost_history = []\n",
    "    for i in range(max_epochs):\n",
    "        if adaptive:\n",
    "            lr = alpha/np.sqrt(i+1)\n",
    "        else:\n",
    "            lr = alpha\n",
    "            \n",
    "        for X,y in minibatches:\n",
    "            z,a = forward_prop(params, X , relu)\n",
    "            grads = back_prop(params, z, a, y , relu, mse)\n",
    "            params = update(params, grads, lr)\n",
    "\n",
    "        cost_history.append(cost_function(a[-1], y))\n",
    "        if i>1 and (abs(cost_history[-1] - cost_history[-2]) < conv):\n",
    "            return params\n",
    "    return params\n",
    "\n",
    "def plot_confusion_matrix(y, y_pred, num, part):\n",
    "    cm_display = ConfusionMatrixDisplay.from_predictions(y_pred.T.argmax(axis = 1), y.T.argmax(axis = 1))\n",
    "    if part=='d':\n",
    "        if num==0:\n",
    "            plt.savefig('ConfusionMatrices/Part{}_Relu.jpg'.format(part))\n",
    "            plt.clf()\n",
    "        else:\n",
    "            plt.savefig('ConfusionMatrices/Part{}_Sigmoid.jpg'.format(part))\n",
    "            plt.clf()\n",
    "                        \n",
    "    else:\n",
    "        plt.savefig('ConfusionMatrices/Part{}_HiddenSize_{}.jpg'.format(part, num))\n",
    "        plt.clf()\n",
    "    \n",
    "def predict(params, X, relu):\n",
    "    z,a = forward_prop(params,X, relu)\n",
    "    y_pred = a[-1]\n",
    "    m = y_pred.shape[1]\n",
    "    for i in range(m):\n",
    "        maximum = np.max(y_pred[:,i])\n",
    "        for j in range(10):\n",
    "            if(y_pred[j,i]==maximum):\n",
    "                y_pred[j,i]=1\n",
    "            else:\n",
    "                y_pred[j,i]=0\n",
    "    return y_pred\n",
    "\n",
    "def plot_graphs(train_accuracies, test_accuracies, training_times, part):\n",
    "    layers = [5,10,15,20,25]\n",
    "    plt.title(\"Accuracies vs Layer Size\")\n",
    "    plt.xlabel(\"Hidden layer size(units)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(layers, train_accuracies)\n",
    "    plt.plot(layers, test_accuracies)\n",
    "    plt.legend([\"Training Accuracies\", \"Test Accuracies\"])\n",
    "    plt.savefig(\"Plots/Part{}_Accuracies.png\".format(part))\n",
    "    plt.clf()\n",
    "                \n",
    "    plt.title(\"Training Time vs Layer Size\")\n",
    "    plt.xlabel(\"Hidden layer size(units)\")\n",
    "    plt.ylabel(\"Training Time\")\n",
    "    plt.plot(layers, training_times)\n",
    "    plt.legend([\"Training Time\"])\n",
    "    plt.savefig(\"Plots/Part{}_TrainingTime.png\".format(part))\n",
    "    plt.clf()\n",
    "    \n",
    "def train_and_test(batch_size, layer_dims, max_epochs, alpha, paths, i, adaptive, conv, part, mse):\n",
    "    if (part=='d' or part=='e' or part=='f') and i==0:\n",
    "        relu = True\n",
    "    else:\n",
    "        relu = False\n",
    "    X_train, y_train= get_data(paths[0])\n",
    "    minibatches = minibatch_create(X_train, y_train, batch_size)\n",
    "    params = init_params(layer_dims)\n",
    "    begin = time.time()\n",
    "    params = model(max_epochs, minibatches, params, alpha, adaptive, conv, relu, mse)\n",
    "    time_to_train = time.time() - begin\n",
    "    y_pred = predict(params, X_train, relu)\n",
    "    acc_train = accuracy(y_train,y_pred)\n",
    "    \n",
    "    X_test, y_test= get_data(paths[1])\n",
    "    y_pred_test = predict(params, X_test, relu)\n",
    "    acc_test = accuracy(y_test,y_pred_test)\n",
    "    \n",
    "    if part=='d':\n",
    "        plot_confusion_matrix(y_test, y_pred_test, i, part)\n",
    "    else:\n",
    "        plot_confusion_matrix(y_test, y_pred_test, 5*(i+1), part)\n",
    "    \n",
    "    return acc_train, acc_test, time_to_train\n",
    "\n",
    "def part_b(paths):\n",
    "    batch_size = 100\n",
    "    alpha = 0.1\n",
    "    max_epochs = 1000\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    training_times = []\n",
    "    hidden_layer_vals = [[5], [10], [15], [20], [25]]\n",
    "    adaptive = False \n",
    "    conv_criteria = 1e-6\n",
    "    lines = []\n",
    "\n",
    "    for i in range(len(hidden_layer_vals)):\n",
    "        layer_dims = [784] + hidden_layer_vals[i] + [10]\n",
    "        acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, i, adaptive, conv_criteria, part, True)\n",
    "        train_accuracies.append(acc_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        training_times.append(time_to_train)\n",
    "        lines.append(\"For hidden layer size {}: \\n\".format(hidden_layer_vals[i][0]))\n",
    "        lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "        lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "        lines.append(\"Time taken to train = \" + str(np.round(time_to_train,2)) + \" seconds\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "        \n",
    "    plot_graphs(train_accuracies, test_accuracies, training_times, part)\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def part_c(paths):\n",
    "    batch_size = 100\n",
    "    alpha = 0.1\n",
    "    max_epochs = 1000\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    training_times = []\n",
    "    hidden_layer_vals = [[5], [10], [15], [20], [25]]\n",
    "    adaptive = True\n",
    "    conv_criteria = 1e-6\n",
    "\n",
    "    for i in range(len(hidden_layer_vals)):\n",
    "        layer_dims = [784] + hidden_layer_vals[i] + [10]\n",
    "        acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, i, adaptive, conv_criteria, part, True)\n",
    "        train_accuracies.append(acc_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        training_times.append(time_to_train)\n",
    "        lines.append(\"For hidden layer size {}: \\n\".format(hidden_layer_vals[i][0]))\n",
    "        lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "        lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "        lines.append(\"Time taken to train = \" + str(np.round(time_to_train,2)) + \" seconds\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "        \n",
    "    plot_graphs(train_accuracies, test_accuracies, training_times, part)\n",
    "    \n",
    "    return lines\n",
    "                    \n",
    "def part_d(paths):\n",
    "    batch_size = 100\n",
    "    alpha = 0.1\n",
    "    max_epochs = 1000\n",
    "\n",
    "    hidden_layer = [100,100]\n",
    "    adaptive = True\n",
    "    conv_criteria = 1e-6\n",
    "\n",
    "    layer_dims = [784] + hidden_layer + [10]\n",
    "    acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, 0, adaptive, conv_criteria, 'd', True)\n",
    "\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(\"For Activation {}: \\n\".format(\"Relu\"))\n",
    "    lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "    lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    print(0)\n",
    "                    \n",
    "    acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, 1, adaptive, conv_criteria, 'd', True)\n",
    "\n",
    "    lines.append(\"For Activation {}: \\n\".format(\"Sigmoid\"))\n",
    "    lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "    lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def part_e(paths):\n",
    "    batch_size = 100\n",
    "    alpha = 0.1\n",
    "    max_epochs = 1000\n",
    "\n",
    "    hidden_layer_vals = [[50,50], [50,50,50], [50,50,50,50], [50,50,50,50,50]]\n",
    "    adaptive = True\n",
    "    conv_criteria = 1e-6\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"For Relu\\n\")\n",
    "    for i in range(len(hidden_layer_vals)):\n",
    "        layer_dims = [784] + hidden_layer_vals[i] + [10]\n",
    "        acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, 0, adaptive, conv_criteria, 'e', True)\n",
    "        train_accuracies.append(acc_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        lines.append(\"For Number of Layers {}: \\n\".format(len(hidden_layer_vals[i])))\n",
    "        lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "        lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "        \n",
    "    layers = [2,3,4,5]\n",
    "    plt.title(\"Accuracies vs Number of Layers\")\n",
    "    plt.xlabel(\"Number of Layers\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(layers, train_accuracies)\n",
    "    plt.plot(layers, test_accuracies)\n",
    "    plt.legend([\"Training Accuracies\", \"Test Accuracies\"])\n",
    "    plt.savefig(\"Plots/Part_e_Accuracies_Relu.png\")\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    lines.append(\"For Sigmoid\\n\")\n",
    "    for i in range(len(hidden_layer_vals)):\n",
    "        layer_dims = [784] + hidden_layer_vals[i] + [10]\n",
    "        acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, 1, adaptive, conv_criteria, 'e', True)\n",
    "        train_accuracies.append(acc_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        lines.append(\"For Number of Layers {}: \\n\".format(len(hidden_layer_vals[i])))\n",
    "        lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "        lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "        lines.append(\"\\n\")\n",
    "        print(0)\n",
    "        \n",
    "    layers = [2,3,4,5]\n",
    "    plt.title(\"Accuracies vs Number of Layers\")\n",
    "    plt.xlabel(\"Number of Layers\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(layers, train_accuracies)\n",
    "    plt.plot(layers, test_accuracies)\n",
    "    plt.legend([\"Training Accuracies\", \"Test Accuracies\"])\n",
    "    plt.savefig(\"Plots/Part_e_Accuracies_Sigmoid.png\")\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "def part_f(paths):\n",
    "    batch_size = 100\n",
    "    alpha = 0.1\n",
    "    max_epochs = 1000\n",
    "\n",
    "    #change this\n",
    "    hidden_layer = [100,100,100]\n",
    "    adaptive = True\n",
    "    conv_criteria = 1e-6\n",
    "\n",
    "    layer_dims = [784] + hidden_layer + [10]\n",
    "    acc_train, acc_test, time_to_train = train_and_test(batch_size, layer_dims, max_epochs, alpha ,paths, 0, adaptive, conv_criteria, 'f', False)\n",
    "\n",
    "    lines.append(\"For BCE: \\n\")\n",
    "    lines.append(\"Training Accuracy = \" + str(np.round(acc_train,2)) + \" percent\\n\")\n",
    "    lines.append(\"Test Accuracy = \" + str(np.round(acc_test,2)) + \" percent\\n\")\n",
    "    lines.append(\"\\n\")\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def part_g(paths):\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c670d627",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m paths \u001b[38;5;241m=\u001b[39m [train_path,test_path]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#part_bc(paths, 'b')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#part_bc(paths, 'c')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpart_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mpart_d\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m    235\u001b[0m conv_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m    237\u001b[0m layer_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m784\u001b[39m] \u001b[38;5;241m+\u001b[39m hidden_layer \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m--> 238\u001b[0m acc_train, acc_test, time_to_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_criteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor Activation \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy = \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(acc_train,\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m percent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(batch_size, layer_dims, max_epochs, alpha, paths, i, adaptive, conv, part, mse)\u001b[0m\n\u001b[1;32m    184\u001b[0m params \u001b[38;5;241m=\u001b[39m init_params(layer_dims)\n\u001b[1;32m    185\u001b[0m begin \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 186\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madaptive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m time_to_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m begin\n\u001b[1;32m    188\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict(params, X_train, relu)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(max_epochs, minibatches, params, alpha, adaptive, conv, relu, mse)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m minibatches:\n\u001b[1;32m    127\u001b[0m     z,a \u001b[38;5;241m=\u001b[39m forward_prop(params, X , relu)\n\u001b[0;32m--> 128\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mback_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     params \u001b[38;5;241m=\u001b[39m update(params, grads, lr)\n\u001b[1;32m    131\u001b[0m cost_history\u001b[38;5;241m.\u001b[39mappend(cost_function(a[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y))\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mback_prop\u001b[0;34m(params, z, a, y, relu, mse)\u001b[0m\n\u001b[1;32m     91\u001b[0m m \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[0;32m---> 94\u001b[0m     dA \u001b[38;5;241m=\u001b[39m outer_loss(mse, y, a[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     delta[i] \u001b[38;5;241m=\u001b[39m dA \u001b[38;5;241m*\u001b[39m derivative(z[i], relu, i\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m     96\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mdot(delta[i], a[i]\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m/\u001b[39mm)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_path = 'fmnist_train.csv'\n",
    "test_path = 'fmnist_test.csv'\n",
    "paths = [train_path,test_path]\n",
    "output_path = 'Output/'\n",
    "part = 'd'\n",
    "\n",
    "part_dict = {'b':part_b, 'c':part_c, 'd': part_d, 'e': part_e, 'f':part_f, 'g': part_g}\n",
    "\n",
    "result = part_dict[part](paths)\n",
    "\n",
    "file = open(output_path + part+\".txt\", \"w\")\n",
    "file.writelines(result)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2836d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes = (50,50,50), \n",
    "                    solver= 'sgd', \n",
    "                    batch_size = 100,\n",
    "                    learning_rate = 'adaptive',\n",
    "                    learning_rate_init = 0.1,\n",
    "                    random_state=1, \n",
    "                    max_iter=300).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
