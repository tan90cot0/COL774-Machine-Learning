{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213c3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396c26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"train.csv\"\n",
    "validation_data_path = \"val.csv\"\n",
    "test_data_path = \"test.csv\"\n",
    "paths = (train_data_path, validation_data_path, test_data_path)\n",
    "output_folder_path = \"output\"\n",
    "question_number = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0934208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ignore(data):\n",
    "    y = data[\"Severity\"]\n",
    "    X = data[[\"Age\", \"Shape\", \"Margin\", \"Density\"]]\n",
    "    X = X.to_numpy()\n",
    "    to_remove = set()\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i][j]==\"?\":\n",
    "                to_remove.add(i)\n",
    "            else:\n",
    "                X[i][j] = int(X[i][j])\n",
    "    X = np.delete(X, list(to_remove), axis = 0)\n",
    "    y = y.to_numpy()\n",
    "    y = np.delete(y, list(to_remove), axis = 0)\n",
    "    return X,y\n",
    "\n",
    "def get_data_impute(data, train, imp, func):\n",
    "    y = data[\"Severity\"]\n",
    "    X = data[[\"Age\", \"Shape\", \"Margin\", \"Density\"]]\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i][j]==\"?\":\n",
    "                X[i][j] = np.nan\n",
    "            else:\n",
    "                X[i][j] = int(X[i][j])\n",
    "    if train:\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy=func)\n",
    "        imp.fit(X)\n",
    "    X = imp.transform(X)\n",
    "\n",
    "    return X,y, imp\n",
    "\n",
    "def get_data_xgboost(data):\n",
    "    y = data[\"Severity\"]\n",
    "    X = data[[\"Age\", \"Shape\", \"Margin\", \"Density\"]]\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i][j]==\"?\":\n",
    "                X[i][j] = np.nan\n",
    "            else:\n",
    "                X[i][j] = int(X[i][j])\n",
    "    return X,y\n",
    "\n",
    "def get_data(paths, part, func):\n",
    "    train_data_path, validation_data_path, test_data_path = paths\n",
    "    \n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    validation_data = pd.read_csv(validation_data_path)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "    if part=='a' or part=='b' or part=='c' or part=='d':\n",
    "        X_train,y_train = get_data_ignore(train_data)\n",
    "        X_validation,y_validation = get_data_ignore(validation_data)\n",
    "        X_test,y_test = get_data_ignore(test_data)   \n",
    "    elif part=='e':\n",
    "        X_train,y_train, imp = get_data_impute(train_data, True, None, func)\n",
    "        X_validation,y_validation, imp = get_data_impute(validation_data, False, imp, func)\n",
    "        X_test,y_test, imp = get_data_impute(test_data, False, imp, func)\n",
    "    else:\n",
    "        X_train,y_train = get_data_xgboost(train_data)\n",
    "        X_validation,y_validation = get_data_xgboost(validation_data)\n",
    "        X_test,y_test = get_data_xgboost(test_data)\n",
    "\n",
    "    train_data = (X_train, y_train)\n",
    "    validation_data = (X_validation, y_validation)\n",
    "    test_data = (X_test, y_test)\n",
    "    \n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "def predict(clf, X,y):\n",
    "    m,n = X.shape\n",
    "    y_pred = np.zeros(m)\n",
    "    for i in range(y.shape[0]):\n",
    "        y_pred[i] = clf.predict(X[i].reshape(1,n))\n",
    "    acc = (y_pred==y).sum()/m\n",
    "    return acc\n",
    "\n",
    "def visualise(clf, name):\n",
    "    dot_data = tree.export_graphviz(clf, out_file=None, filled = True) \n",
    "    graph = graphviz.Source(dot_data, format = \"png\") \n",
    "    graph.render(name) \n",
    "    \n",
    "def plot_alpha_graphs(alphas, metrics, scores):\n",
    "    train_scores, validation_scores, test_scores = scores\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"Accuracy vs alpha for training and testing sets\")\n",
    "    plt.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\n",
    "    plt.plot(ccp_alphas, validation_scores, marker=\"o\", label=\"validation\", drawstyle=\"steps-post\")\n",
    "    plt.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    impurities, nodes, depth = metrics\n",
    "    \n",
    "    plt.plot(alphas, impurities)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(alphas, nodes)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(alphas, depth)\n",
    "    plt.show()\n",
    "\n",
    "def get_scores(data, clfs):\n",
    "    train_data, validation_data, test_data = data\n",
    "    X_train, y_train = train_data\n",
    "    X_validation, y_validation = validation_data\n",
    "    X_test, y_test = test_data \n",
    "    train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "    validation_scores = [clf.score(X_validation, y_validation) for clf in clfs]\n",
    "    test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "    \n",
    "    return train_scores, validation_scores, test_scores\n",
    "\n",
    "def write_to_file(lines, part):\n",
    "    with open('outputs/1_' + part + '.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "\n",
    "def model(clf, data):\n",
    "    X_train,y_train = data[0]\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "    return acc\n",
    "\n",
    "def part_a(data):\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 0)\n",
    "    acc = model(clf, data)\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    visualise(clf, \"Trees/part_a\")\n",
    "    lines = [\"Results for Part a:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines, clf\n",
    "    \n",
    "def get_part_b_params(train_data):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    param_dict = {\"max_depth\": range(1,5), \n",
    "                 \"min_samples_split\": range(2,100), \n",
    "                 \"min_samples_leaf\": range(1,5)\n",
    "                 }\n",
    "\n",
    "    grid = GridSearchCV(clf, param_grid = param_dict, cv = 10, verbose = 1, n_jobs = -1)\n",
    "    X_train,y_train = train_data\n",
    "    grid.fit(X_train, y_train)\n",
    "    p = grid.best_params_\n",
    "    return p['max_depth'], p['min_samples_leaf'], p['min_samples_split']\n",
    "\n",
    "def part_b(data):\n",
    "    params = get_part_b_params(data[0])\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = params[0], min_samples_leaf = params[1], min_samples_split = params[2])\n",
    "    acc = model(clf, data)\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    visualise(clf, \"Trees/part_b\")\n",
    "    lines = [\"Results for Part b:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines, clf\n",
    "\n",
    "def get_part_c_params(train_data):\n",
    "    X_train, y_train = train_data\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "    path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "    clfs = []\n",
    "    nodes = []\n",
    "    depth = []\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "        clf.fit(X_train, y_train)\n",
    "        clfs.append(clf)\n",
    "        nodes.append(clf.tree_.node_count)\n",
    "        depth.append(clf.tree_.max_depth)\n",
    "\n",
    "    ccp_alphas = ccp_alphas[:-1]\n",
    "    clfs = clfs[:-1]\n",
    "    impurities = impurities[:-1]\n",
    "    nodes = nodes[:-1]\n",
    "    depth = depth[:-1]\n",
    "\n",
    "    metrics = (impurities, nodes, depth)\n",
    "    \n",
    "    return ccp_alphas, clfs, metrics\n",
    "\n",
    "def part_c(data):\n",
    "    ccp_alphas, clfs, metrics = get_part_c_params(data[0])\n",
    "    scores = get_scores(data, clfs)\n",
    "\n",
    "    #plot_alpha_graphs(ccp_alphas, metrics, scores)\n",
    "    index = np.argmax(np.array(scores[1]))\n",
    "    best_alpha, best_tree = ccp_alphas[index], clfs[index]\n",
    "    acc = scores[0][index], scores[1][index], scores[2][index]\n",
    "    \n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    \n",
    "    lines.append('Best Alpha = ' + str(np.round(best_alpha,5)) + '\\n')\n",
    "\n",
    "    visualise(best_tree, \"Trees/part_c\")\n",
    "    lines = [\"Results for Part c:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines, best_tree\n",
    "\n",
    "def get_part_d_params(X,y):\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    param_dict = {\"n_estimators\": range(1,15), \n",
    "                    \"max_features\": [\"sqrt\", \"log2\", None], \n",
    "                    \"min_samples_split\": range(2,3)\n",
    "                    }\n",
    "    #Tested up to 1000 n-estimators\n",
    "    #10 min sample splits\n",
    "\n",
    "    grid = GridSearchCV(clf, param_grid = param_dict, cv = 10, verbose = 1, n_jobs = -1)\n",
    "\n",
    "    grid.fit(X,y)\n",
    "    p = grid.best_params_\n",
    "    return p[\"n_estimators\"], p['min_samples_split'], p['max_features']\n",
    "    \n",
    "def part_d(data):\n",
    "    X_train,y_train = data[0]\n",
    "    params = get_part_d_params(X_train, y_train)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators = params[0], min_samples_split = params[1], max_features = params[2], oob_score = True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    \n",
    "    lines.append('Out of bag Accuracy: ' + str(np.round(clf.oob_score_*100,5)) + '\\n')\n",
    "    lines = [\"Results for Part d:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines, None\n",
    "\n",
    "def part_e(paths):\n",
    "    agg_func = [\"Results for 'mean' aggregate function:\\n\", \"Results for 'median' aggregate function:\\n\"]\n",
    "    funcs = ['mean', 'median']\n",
    "    result = []\n",
    "    for i in range(2):\n",
    "        result+= [agg_func[i]]\n",
    "        data = get_data(paths, 'e', funcs[i])\n",
    "        temp, clf = part_a(data)\n",
    "        result+=temp\n",
    "        temp, clf =part_b(data)\n",
    "        result+=temp\n",
    "        temp, clf =part_c(data)\n",
    "        result+=temp\n",
    "        temp, clf =part_d(data)\n",
    "        result+=temp\n",
    "    return result\n",
    "\n",
    "def best_part_f_params(X,y):\n",
    "    estimator = XGBClassifier(objective= 'binary:logistic', nthread=4,seed=42)\n",
    "    param_dict = {\"n_estimators\": range(10,50,10), \n",
    "                     \"subsample\": [0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "                     \"max_depth\": range(4,10,1)\n",
    "                     }\n",
    "\n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=param_dict, scoring = 'roc_auc', n_jobs = 10, cv = 10, verbose=True)\n",
    "    grid.fit(X,y)\n",
    "    p = grid.best_params_\n",
    "    return p[\"n_estimators\"], p[\"subsample\"], p[\"max_depth\"]\n",
    "\n",
    "def part_f(data):\n",
    "    X_train, y_train = data[0]\n",
    "    params = best_part_f_params(X_train, y_train)\n",
    "    clf = XGBClassifier(objective= 'binary:logistic', nthread=4,seed=42, n_estimators = params[0], subsample = params[1], max_depth = params[2])\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "    \n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abf6dbb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 120 candidates, totalling 1200 fits\n"
     ]
    }
   ],
   "source": [
    "question_number = 'f'\n",
    "l = ['a', 'b', 'c', 'd']\n",
    "part_function = {'a':part_a, 'b':part_b, 'c': part_c, 'd': part_d}\n",
    "\n",
    "if question_number in l:\n",
    "    data = get_data(paths, question_number, None)\n",
    "    result,classifier = part_function[question_number](data)\n",
    "    if question_number!='d':\n",
    "        visualise(classifier)\n",
    "elif question_number=='e':\n",
    "    result = part_e(paths)\n",
    "else:\n",
    "    data = get_data(paths, question_number, None)\n",
    "    result = part_f(data)\n",
    "        \n",
    "write_to_file(result, question_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486ad005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.6.2-py3-none-macosx_12_0_arm64.whl (1.5 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.9.3-cp310-cp310-macosx_12_0_arm64.whl (28.5 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.23.4-cp310-cp310-macosx_11_0_arm64.whl (13.3 MB)\n",
      "Installing collected packages: numpy, scipy, xgboost\n",
      "Successfully installed numpy-1.23.4 scipy-1.9.3 xgboost-1.6.2\n",
      "\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/xgboost-1.6.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/xgboost already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/numpy-1.23.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/scipy-1.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Target directory /Users/Aryan/Library/Python/3.10/lib/python/site-packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86948ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d2849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
