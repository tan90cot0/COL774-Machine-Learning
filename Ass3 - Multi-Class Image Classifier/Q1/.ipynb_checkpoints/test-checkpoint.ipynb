{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a5fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack, csr_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da296920",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"DrugsComTrain.csv\"\n",
    "validation_data_path = \"DrugsComVal.csv\"\n",
    "test_data_path = \"DrugsComTest.csv\"\n",
    "paths = (train_data_path, validation_data_path, test_data_path)\n",
    "output_folder_path = \"output\"\n",
    "question_number = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4046ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(review, stopwords):\n",
    "    tokens = review.lower().replace('&#039;', \"\").replace('.', \" \").replace(',', \" \").replace('\"', \" \").replace('\\r', \" \").replace('\\n', \" \").replace('-', \" \").split()\n",
    "    new_review = \"\"\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            new_review+=word + \" \"\n",
    "    return new_review[:-1]\n",
    "\n",
    "def get_vocab(st):\n",
    "    vectorizer = CountVectorizer()\n",
    "    st = vectorizer.fit_transform(st)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    return st, vocab, vectorizer\n",
    "\n",
    "def extract_string_feature(feature, stop_words, train, vectorizer):\n",
    "    for i in range(len(feature)):\n",
    "        if type(feature[i])==float and math.isnan(feature[i]):\n",
    "            feature[i] = \"\"\n",
    "        else:\n",
    "            feature[i] = clean(feature[i], stop_words)\n",
    "\n",
    "    if train:\n",
    "        return get_vocab(feature)\n",
    "    else:\n",
    "        return vectorizer.transform(feature), None, vectorizer\n",
    "\n",
    "def get_date(l):\n",
    "    months = {\"January\": 1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7, \"August\":8, \"September\":9, \"October\":10, \"November\":11, \"December\":12}\n",
    "    dates = np.zeros((len(l), 3))\n",
    "    for i in range(len(l)):\n",
    "        temp = l[i].split()\n",
    "        month = months[temp[0]]\n",
    "        date = int(temp[1][:-1])\n",
    "        year = int(temp[2])\n",
    "        dates[i] = np.array([date, month, year])\n",
    "    return csr_matrix(dates)\n",
    "\n",
    "def get_arrays(data, train, vect, stop_words):\n",
    "    conditions, cond_vocab, vect[0] = extract_string_feature(list(data[\"condition\"]), stop_words, train, vect[0])\n",
    "    reviews, reviews_vocab, vect[1] = extract_string_feature(list(data[\"review\"]), stop_words, train, vect[1])\n",
    "    dates = get_date(list(data[\"date\"])) \n",
    "    usefulCount = csr_matrix(np.array(data['usefulCount']).reshape(dates.shape[0], 1))\n",
    "    X = hstack([reviews, conditions, dates, usefulCount])\n",
    "    y = np.array(data[\"rating\"])\n",
    "    y = y-1\n",
    "    return (X,y),vect\n",
    "\n",
    "def get_data(paths):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    vect = [None, None]\n",
    "    full_data = []\n",
    "    for i in range(3):\n",
    "        data, vect = get_arrays(pd.read_csv(paths[i]), i==0, vect, stop_words)\n",
    "        full_data.append(data)\n",
    "\n",
    "    return tuple(full_data)\n",
    "\n",
    "def plot_alpha_graphs(alphas, metrics, scores):\n",
    "    train_scores, validation_scores, test_scores = scores\n",
    "    plt.xlabel(\"Alphas\")\n",
    "    plt.ylabel(\"Accuracies\")\n",
    "    plt.title(\"Accuracy vs Alpha for Training and Test sets\")\n",
    "    plt.plot(alphas, train_scores, marker=\"o\", label=\"Train\", drawstyle=\"steps-post\")\n",
    "    plt.plot(alphas, validation_scores, marker=\"o\", label=\"Validation\", drawstyle=\"steps-post\")\n",
    "    plt.plot(alphas, test_scores, marker=\"o\", label=\"Test\", drawstyle=\"steps-post\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Plots/Dataset2/Accuracy')\n",
    "    plt.clf()\n",
    "\n",
    "    impurities, nodes, depth = metrics\n",
    "    \n",
    "    plt.xlabel(\"Alphas\")\n",
    "    plt.ylabel(\"Impurities\")\n",
    "    plt.title(\"Impurity vs Alpha\")\n",
    "    plt.plot(alphas, impurities, marker=\"o\", label=\"Impurities\", drawstyle=\"steps-post\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Plots/Dataset2/Impurity')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Alphas\")\n",
    "    plt.ylabel(\"Nodes\")\n",
    "    plt.title(\"Nodes vs Alpha\")\n",
    "    plt.plot(alphas, nodes, marker=\"o\", label=\"Nodes\", drawstyle=\"steps-post\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Plots/Dataset2/Nodes')\n",
    "    plt.clf()\n",
    "\n",
    "    plt.xlabel(\"Alphas\")\n",
    "    plt.ylabel(\"Depth\")\n",
    "    plt.title(\"Depth vs Alpha\")\n",
    "    plt.plot(alphas, depth, marker=\"o\", label=\"Depth\", drawstyle=\"steps-post\")\n",
    "    plt.legend()\n",
    "    plt.savefig('Plots/Dataset2/Depth')\n",
    "    plt.clf()\n",
    "\n",
    "def model(clf, data):\n",
    "    X_train,y_train = data[0]\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "    return acc\n",
    "\n",
    "def part_a(data):\n",
    "    X_train, y_train = data[0]\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 0)\n",
    "    acc = model(clf, data)\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    lines = [\"Results for Part a:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines\n",
    "\n",
    "def get_part_b_params(train_data):\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    param_dict = {\"max_depth\": range(1,5), \n",
    "                 \"min_samples_split\": range(2,10), \n",
    "                 \"min_samples_leaf\": range(1,5)\n",
    "                 }\n",
    "\n",
    "    grid = GridSearchCV(clf, param_grid = param_dict, cv = 10, verbose = 1, n_jobs = -1)\n",
    "    X_train,y_train = train_data\n",
    "    grid.fit(X_train, y_train)\n",
    "    p = grid.best_params_\n",
    "    return p['max_depth'], p['min_samples_leaf'], p['min_samples_split']\n",
    "\n",
    "def part_b(data):\n",
    "    params = get_part_b_params(data[0])\n",
    "    X_train, y_train = data[0]\n",
    "    clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = params[0], min_samples_leaf = params[1], min_samples_split = params[2])\n",
    "    acc = model(clf, data)\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    lines = [\"Results for Part b:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines\n",
    "\n",
    "def get_scores(data, clfs):\n",
    "    train_data, validation_data, test_data = data\n",
    "    X_train, y_train = train_data\n",
    "    X_validation, y_validation = validation_data\n",
    "    X_test, y_test = test_data \n",
    "    train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "    validation_scores = [clf.score(X_validation, y_validation) for clf in clfs]\n",
    "    test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "    \n",
    "    return train_scores, validation_scores, test_scores\n",
    "\n",
    "def get_part_c_params(train_data):\n",
    "    X_train, y_train = train_data\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "    path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "    clfs = []\n",
    "    nodes = []\n",
    "    depth = []\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "        clf.fit(X_train, y_train)\n",
    "        clfs.append(clf)\n",
    "        nodes.append(clf.tree_.node_count)\n",
    "        depth.append(clf.tree_.max_depth)\n",
    "\n",
    "    ccp_alphas = ccp_alphas[:-1]\n",
    "    clfs = clfs[:-1]\n",
    "    impurities = impurities[:-1]\n",
    "    nodes = nodes[:-1]\n",
    "    depth = depth[:-1]\n",
    "\n",
    "    metrics = (impurities, nodes, depth)\n",
    "    \n",
    "    return ccp_alphas, clfs, metrics\n",
    "\n",
    "def part_c(data):\n",
    "    ccp_alphas, clfs, metrics = get_part_c_params(data[0])\n",
    "    scores = get_scores(data, clfs)\n",
    "\n",
    "    plot_alpha_graphs(ccp_alphas, metrics, scores)\n",
    "    index = np.argmax(np.array(scores[1]))\n",
    "    best_alpha, best_tree = ccp_alphas[index], clfs[index]\n",
    "    acc = scores[0][index], scores[1][index], scores[2][index]\n",
    "    \n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    lines.append('Best Alpha = ' + str(np.round(best_alpha,5)) + '\\n')\n",
    "    lines = [\"Results for Part c:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines\n",
    "\n",
    "def get_part_d_params(X,y):\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    #check max features\n",
    "    l = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    param_dict = {\"n_estimators\": range(50,450, 50), \n",
    "                    \"max_features\": [\"sqrt\", \"log2\", None], \n",
    "                    \"min_samples_split\": range(2,10,2)\n",
    "                    }\n",
    "    grid = GridSearchCV(clf, param_grid = param_dict, cv = 10, verbose = 1, n_jobs = -1)\n",
    "    grid.fit(X,y)\n",
    "    p = grid.best_params_\n",
    "    return p[\"n_estimators\"], p['min_samples_split'], p['max_features']\n",
    "\n",
    "def part_d(data):\n",
    "    X_train,y_train = data[0]\n",
    "    params = get_part_d_params(X_train, y_train)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators = params[0], min_samples_split = params[1], max_features = params[2], oob_score = True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "\n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    \n",
    "    lines.append('Out of bag Accuracy: ' + str(np.round(clf.oob_score_*100,5)) + '\\n')\n",
    "    lines = [\"Results for Part d:\\n\"] + lines + [\"\\n\"]\n",
    "    return lines\n",
    "\n",
    "def best_part_e_params(X,y):\n",
    "    estimator = XGBClassifier(objective= 'binary:logistic', nthread=4,seed=42)\n",
    "    l = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    param_dict = {\"n_estimators\": range(50,450,50), \n",
    "                     \"subsample\": l, \n",
    "                     \"max_depth\": range(40,70,10)\n",
    "                     }\n",
    "\n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=param_dict, scoring = 'roc_auc', n_jobs = 10, cv = 10, verbose=True)\n",
    "    grid.fit(X,y)\n",
    "    p = grid.best_params_\n",
    "    return p[\"n_estimators\"], p[\"subsample\"], p[\"max_depth\"]\n",
    "\n",
    "def part_e(data):\n",
    "    X_train, y_train = data[0]\n",
    "    params = best_part_e_params(X_train, y_train)\n",
    "    clf = XGBClassifier(objective= 'binary:logistic', nthread=4,seed=42, n_estimators = params[0], subsample = params[1], max_depth = params[2])\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = []\n",
    "    for X,y in data:\n",
    "        acc.append(clf.score(X,y))\n",
    "    \n",
    "    lines = ['Training Accuracy is: ', 'Validation Accuracy is: ', 'Test Accuracy is: ']\n",
    "    lines = [lines[i] + str(np.round(acc[i]*100,5)) + '\\n' for i in range(len(lines))]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31da2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Results for Part a:\\n', 'Training Accuracy is: 100.0\\n', 'Validation Accuracy is: 58.10618\\n', 'Test Accuracy is: 57.80233\\n', '\\n']\n",
      "247.66923117637634\n"
     ]
    }
   ],
   "source": [
    "data = get_data(paths)\n",
    "begin = time.time()\n",
    "result = part_a(data)\n",
    "print(result)\n",
    "print(time.time()-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f33174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
